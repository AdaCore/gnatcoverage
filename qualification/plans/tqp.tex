\documentclass {report}
\usepackage{couverture}

\usepackage{color}
\definecolor{light-gray}{gray}{0.85}
\usepackage{listings}
\lstset{backgroundcolor=\color{light-gray}}

\begin{document}
\title{\huge
  \xcov{}\\
  Tool Qualification Plan\\ \ \\
  \large \textbf{Document Version 1.0}}

\maketitle
\tableofcontents

\chapter{Document introduction}

\section{Document purpose}
The purpose of this document is to describe the applicable processes to
qualify \xcov{} in a DO-178B and DO-178C context.

\section{Authors}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Name} & \textbf{Company} & \textbf{Email} \\ \hline
Matteo Bordin & AdaCore & bordin@adacore.com \\ \hline
Olivier Hainque & AdaCore & hainque@adacore.com \\ \hline
\end{tabular}

\section{Major revision history}

The evolution of this document is automatically tracked by the configuration
management system. Here we just provide the major revision history.  \ \\ \\
\begin{tabular}{|c|c|c|}
\hline
\textbf{Version} & \textbf{Date} & \textbf{Comment} \\ \hline
 &  &  \\ \hline
\end{tabular}

\section{Ongoing work}

\begin{itemize}
\item%
  Need to integrate big blob on the tor/testbase orgnaisation.
%
\item%
  Need to clarify the notion of qualified interface vs command line
  options and coding standard
%
\item%
  Need to fill missing briefs on the various ``Plan'' documents in
  the Qualification Data chapter
%
\item%
  Need to complete/clarify the Preparing for Qualified Use chapter,
  in particular who does and produces what (e.g. who runs the tests etc).
\end{itemize}

% **************************************************************************
\include{doc_introduction}

% **************************************************************************
\chapter{Tool Overview \& Qualified Interface}

\section{Tool overview}

\xcov{} is a tool designed to assess the object or source level coverage
achieved by a testing campaign on software.
%
\xcov{} operates without any instrumentation of the original program, on
code generated by the same compiler as the one producing the executable
deployed on the target hardware.

Source level structural coverage assessments are achieved with a three-steps
process outlined in the qualified interface section of this document
(\ref{sec:qual-interface}).
%
\xcov{} analyses two core pieces of information in this process:

\begin{Itemize}
%
\item The list of object instructions executed by the test-sets of interest,
  also known as \E{execution traces}, produced by an instrumented execution
  environment;
%
\item Tables that allow mapping machine instructions to precise source
  constructs (statements, decisions, conditions), produced by the compiler.
\end{Itemize}

Detailed information on the tool capabilities and characteristics is available
in the ``\xcov{} Fundamentals and Users Guide'' as well as in \adaeurope and
\erts.
%
We use an instrumented version of the \qemu{} open-source processor simulator
as the execution environment.

\section {Sought certification credit}

\xcov{} aims at automating the structural coverage assessment activities
required by the Software Verification Process of DO-178.

Such assessments are mandatory for levels A, B, and C, with more stringent
requirements for the more critical levels (table A7, objectives 5-7): \mcdc{},
\dc{} and \stc{} for level A ; \dc{} and \stc{} for level B, and \stc{} for
 level C.

\xcov{} is qualified as a \emph{verification tool}, with a Tool Qualification
Level 5 (TQL5) in DO-178C terms.
%
Consequently, as long as uses comply with the qualified interface described in
section \ref{sec:qual-interface}, applicants may use \xcov{}'s output as
certification evidence and certification authorities shall consider \xcov{}'s
output \emph{as good as} the output of a manual activity.

\section{Qualified interface}
\label{sec:qual-interface}

To produce reports suitable for use as certification evidence, \xcov{} shall
be used as follows:

\begin{enumerate}
\item Build the executable application using the GNAT Pro toolchain identified
  in the delivery file (see section \ref{sec:delivery}), obeying the
  compilation options and coding standard documented there.

\item To produce an execution trace file \T{<TRACE>}, run the \T{<APP>}
  executable program within the instrumented execution environment using the
  following command line:\\

\T{xcov run --target=<TARGET> --level=<LEVEL> <APP> -o <TRACE>} \\

... where \T{<TARGET>} identifies the target platform (as in the GNAT Pro
toolchain prefixes, e.g.  powerpc-elf), and \T{<LEVEL>} designates the
coverage criteria to assess: \T{stmt}, \T{stmt+decision} or \T{stmt+mcdc};

\item To produce the assessment \T{<REPORT>} file, feed the execution traces
  of interest to the coverage analyzer using the following command line:\\

\texttt{xcov coverage --annotate=report --level=<LEVEL> \\
  --scos=@alis.list @traces.list -o <REPORT>} \\

... where \T{<LEVEL>} is the same as for \T{xcov run}, \T{traces.list} is a
file containing the list of execution traces to operate on, and \T{alis.list}
is a file containing the list of GNAT Pro ALI file names corresponding to the
source units for which coverage is to be assessed.

When the \T{traces.list} input lists multiple traces, \xcov{} consolidates the
coverage achieved by all the corresponding executions as if they all happened
in sequence as a single one.

The output report format is documented in an appendix of the \xcov{}
Fundamentals and Users Guide.

\end{enumerate}


% **************************************************************************
\chapter{Tool development and verification standards}

\section{General directions}

To elaborate how the tool should behave and assess that it produces reliable
outputs, useable as certification evidence, we provide:

\begin{Itemize}
\item%
  An explicit description of the expected behavior as a set of \term{Tool
    Operational Requirements} (a.k.a \term{TORs}) for nominal use conditions,
  and
\item%
  A set of executable \term{Test Cases} (a.k.a \term{TCs}) associated with
  each requirement, exercized to validate that the behavior indeed corresponds
  to expectations.
\end{Itemize}

Multiple aspects of the tool behavior have to be considered, and we
distinguish different categories of tool operational requirements for this
purpose.
%
To start with:

\begin{enumerate}
\item \textbf{Coverage of isolated source constructs.}
%
This category focuses on the users' expectations for the coverage of
specific source constructs (if statements, loop, blocks, etc) with respect to
a particular coverage metric.
%
\xcov{}'s qualified output focuses on cases of absence of (or partial)
coverage, so requirements of this category typically express conditions when
coverage shall be reported as \emph{not} achieved;
%
for example: \emph{"A boolean decision exercized only one way shall be
reported as only partially covered"}.

Individual requirements in this category target a single language construct.
%
The use of programming language reference manuals provides a significant
contribution in identifying the constructs of interest.

Testcases for this kind of TORs typically exercise the same construct in
various ways, for example by causing a boolean expression to be evaluated just
to True or False, and verify that results are are expected in all the
variations.

\item \textbf{General coverage analysis facilities.}
%
This category focuses on general facilities offered by the tool, not at all
tied to a particular language construct, such as the support for coverage
exemptions, or consolidation capabilities.

Testcases for requirements in this category play two roles: validate the tool
behavior with respect to the stated requirement, of course, and exercize the
tool on code where mutliple language features are mixed together.
%
Having tests for such mixes is necessary and there is no constraint on this
account here, so we take the opportunity.
% 
And in any case, the facilities to validate typically require a mix of
constructs to be tested properly.
\end{enumerate}

For those two first categories, our principal focus is to assess that \xcov{}
is \emph{sound}, that is, to make sure that violations of a given coverage
criterion are detected.
%
Nevertheless, our testsuite driver expects a strict one-to-one match between
result expectations stated by test writers in testcases and the diagnostics
emitted by the tool, such that every reported violation must have been stated
as expected and every expected violation must be reported for a test to pass.

Beyond all that, we have identified two other requirement categories of use:

\begin{enumerate}
\item \textbf{Compliance of the output report with the documented format.}
%
The documented output report format is a part of the tool qualified interface.
% 
This category of requirements focuses on well identified aspects of that
report format.

Dedicated testcases are designed to verify that all the mandatory pieces are
there.
%
Part of these requirements are also implicitly validated by the execution of
all the coverage checking testcases in other categories, where specific
sections of the report are scanned to search for criteria violation messages
and compare them with user specified expectations.

\item \textbf{Representativeness of the coverage assessment environment.}
%
Eventhough compiled with the final target compiler, the qualification Test
Cases are not bound to run and be tested on the final hardware as pieces of
the applicative system.
%
This category of requirements aims at validating that the instrumented
execution environment used by \xcov{} to assess coverage is representative of
the final hardware.

TORs in this category are validated by testcases comparing the output of
executables run via \xcov{} and on the final target processor, together with
the addition of functional assertions in all the testcases of the other
categories.
%
Such assertions may be viewed as partial specifications of the final target
hardware expected behavior, which are designed to cause test failures when the
assessment environment behavior deviates from the target model.
\end{enumerate}

\section{Operational organization}

We qualify a well identified \xcov{}/\qemu{} bundle to perform structural
coverage assessments in accordance with the qualified interface description in
section \ref{sec:qual-interface}.

We focus on the \T{--annotate=report} text output of xcov, which exposes a
list of violations with respect to a given coverage criterion, diagnostics
such as "statement not covered at <file>:<line>:<col>" or alike.

The suite driver uses the \T{PATH} environment variable to select the tools
and reports the corresponding versions. This is always relevant for xcov and
qemu.
%
The compiler version and command line options are reported as well, which is
relevant if the binary form of the user applicative code used for coverage
assessment is to be embedded "as-is".

Tool Operational Requirements, Test Cases and the associated Tests are
all collected and linked together in a filesystem tree, version controlled
through a configuration management repository.
%
A set of strict organizational rules is enforced to ensure global consistency,
traceability, and allow automated execution of the tests by a testsuite driver
to assess the tool behavior correctness.

\subsection{Toplevel entry points}

All the artifacts are hosted in the Qualif/<lang> directory subtree, with
three first level entries:

\begin{Itemize}
\item \texttt{stmt} for requirements related to statement coverage
   capabilities,
%
\item \texttt{decision} for requirements oriented towards decision coverage
   capabilities, and
%
\item \texttt{mcdc} for requirements specific to mcdc coverage assessments.
\end{Itemize}

Qualification for one \do{} level requires execution of all the relevant
categories of tests: \texttt{stmt} for level C, \texttt{stmt} \& \T{decision}
for level B, \T{stmt}, \T{decision} \& \T{mcdc} for level A.

Each of these toplevel subdirectories is then subdivided according to
the following rules:

\subsection{Tool Operational Requirements}

Each TOR maps to a physical folder in the repository.
%
The folder name is the TOR identifier.
%
All the qualification artifacts related to a TOR are stored as either files or
subfolders within the TOR directory:

Every TOR directory holds a \T{req.txt} file where the TOR textual
specification resides.
%
The TOR is then either validated by one or more testcases, or subdivided
as a set of simpler TORs as needed.

Each testcase materializes as a subdirectory, organized as described in
the following section.
%
When a TOR is validated by several testcases, the \T{req.txt} specification
shall include a ``Testing strategy'' section which provides a general
description of how distinct aspects of the requirement are fullfilled by each
testcase.

Sub-TORs are hosted as subdirectories as well and obey the same rules as
their parent.

\subsection{Test Cases \& Tests}

A Test Case is set of Tests aimed at validating part or all of an operational
requirement.
%
Each test case associated with a TOR is held in a subdirectory of the TOR
folder.
%
The subdirectory name is the testcase identifier.

Every test case subdirectory shall contain a \T{tc.txt} file, which holds
a textual description of the test case intent and organization.
%
When this is one of multiple test cases for a requirement, this description
completes the general comments found in the TOR Testing Strategy notes.

The sources for a test case always involve two categories of sources:

\begin{Itemize}
\item%
  Functional sources, code for which some coverage properties are to be
  assessed,
%
\item%
  Driver sources, which invoke the functional code in various ways and embed a
  description of the expected coverage outcome.
\end{Itemize}

File names starting with \T{test\_} identify driver sources. Multiple drivers
may be used to exercise a single functional source.

Every test case has specific sources, always located in the \T{src}
subdirectory of the TC folder.
%
Sources may be shared between test cases, for either functional or driver
code, searched in \T{src} subdirectories of parent folders as needed.

Section \ref{sec:assessment} explains how the testsuite engine
locates and executes the applicable drivers for a test case.
%
Section \ref{sec:tc-dev} provides details on how test case development
proceeds, and in particular how coverage expectations are specified.

The following section first offers a synthetic summary of the general
tree organization we have described:

\subsection{Synthesis}

The schematic representations that follow summarize dummy TOR/TC tree
structures to illustrate the general organization scheme.

First, the toplevel entries, then a couple of TORs with two standalone test
cases associated with the first one:

\begin{verbatim}
 Qualif/<language>/stmt
                  /decision
                  /mcdc

 stmt/TOR_1/req.txt (with Testing Strategy notes)
     .     .
     .     /TC_1/tc.txt
     .     .    /src/foo.adb
     .     .        /test_foo_1.adb
     .     .        /test_foo_2.adb
     .     .
     .     /TC_2/tc.txt
     .          /src/bar.adb
     .              /test_bar_1.adb
     .              /test_bar_2.adb
     .
     /TOR_2/req.txt [...]
     ...
\end{verbatim}


Second, a couple of test cases sharing driver sources, and then a couple of
test cases sharing functional sources:

\begin{verbatim}
 stmt/TOR_7/req.txt 
     .     .
     .     /src/test_foo_1.adb
     .     .   /test_foo_2.adb
     .     .
     .     /TC_1/tc.txt
     .     .    /src/foo.adb
     .     .
     .     /TC_2/tc.txt
     .     .    /src/foo.adb
     ...
\end{verbatim}

\begin{verbatim}
 stmt/TOR_9/req.txt 
     .     .
     .     /src/bar.adb
     .     .
     .     /TC_1/tc.txt
     .     .    /src/test_bar_1.adb
     .     .
     .     /TC_2/tc.txt
     .     .    /src/test_bar_1.adb
     ...
\end{verbatim}


\section{Assessment operation summary}
\label{sec:assessment}

Our testsuite automated execution is Python driven.
%
For a list a infrastructure preriquisites to allow the tests to run, see the
\T{README} file in the suite toplevel dir.

\subsection{Locating and executing Test Cases}

The testsuite engine, invoked from the qualification toplevel directory,
searches for test cases, executes every one it finds, keeps track of
passes/failures as it goes, and produces a synthetic report at the end.

The engine seeks executable \T{test.py} Python files to locate test cases.
%
One \T{test.py} file is expected to be found next to testcase descriptions,
that is, everywhere a \T{tc.txt} resides.

For every test case, the engine first locates the applicable driver sources,
selecting those from the \T{src/} subdirectory, if any, or searching uptree
otherwise.
%
In the latter case, functional sources are expected in \T{src/}, and
the engine searches for corresponding \T{test\_} candidates.

For every \T{test\_<x>} driver, the engine then ...

\begin{Itemize}  
\item%
  Builds the executable program (driver + functional code to exercize),

\item%
  Executes the program with \T{xcov run}, producing an execution trace,

\item%
  Invokes \T{xcov coverage} to analyze the trace and output source coverage
  reports, once with --annotate=xcov and once with --annotate=report,

\item%
  Compare the outputs with the expectations stated in the driver sources,

\item%
  Decides whether they match (test passes) or not (test fails) and report.
\end{Itemize}  

All of this is performed in a separate subdirectory called \T{tmp\_test\_<x>}.

\subsection{Consolidation Scenarii}

The engine then checks whether consolidation scenarii are to be exercized.
%
These are specified as \T{cons\_<scenario\_id>.txt} text files in the \T{src/}
subdirectory, each expected to contain a \T{drivers=<regexp>} line followed by
coverage expectations.

For each consolidation scenario found, the engine consolidates the traces
previously produced by all the drivers whose name matches \T{regexp} (for
example, \T{.}  selects them all), compares the results with the scenario
expectations and reports.

This is all performed in a separate subdirectory called
 \T{tmp\_cons\_<scenario\_id>}.

\subsection{Control Options}

The engine driver supports various options to help developers, like
ways to select subsets of case to exercize or ways to specify alternate
compilation options for expermientation purposes.
%
These are for \xcov{} developers use only.

\section{Test Case development}
\label{sec:tc-dev}

\subsection{Overview}

To contribute a test case for a TOR, developers have to

\begin{Itemize}
\item%
  Create of a dedicated subdirectory in the TOR folder,
%
\item%
  Provide \T{tc.txt}, describing the testcase intent and organization,
%
\item%
  Develop the test case specific sources, providing coverage expectations
  and/or support for them as needed (see in the following text),
%
\item%
  Provide a \T{test.py}, to hook in the testsuite engine.
%
\end{Itemize}

Thanks to the simple source naming conventions and the in-source embedded
expectations, the \T{test.py} contents is entirely generic and can simply be
copied from one test case to the other.
%
The presence of a this file is still useful to help the toplevel driver locate
and launch testcase executions, as outlined in section \ref{sec:assessment}.

\subsection{Coverage Expectations}

The description of expectations on coverage results is achieved with two
devices:

\begin{enumerate}
\item \textbf{In functional sources}:
  %
  Comments starting with "\T{-{}- \#}" on lines for which coverage
  expectations need to be specified. These provide ways to refer
  to functional lines from ...

\item \textbf{In driver sources, at the end}, a sequence of:\\

  \T{-{}-\# <functional\_file\_name>}\\

  followed by optional sequence of:\\

  \T{-{}-  /regexp/ <expected .xcov note> <expected report notes>}\\
\end{enumerate}

In the optional sequence at the end of drive sources:

\begin{Itemize}
\item \T{/regexp/} selects all the source lines matching "\T{-- \# \& regexp}"

\item \T{<expected .xcov note>} denotes the synthesis sign we expect on the
  selected lines in the \T{--annotate=xcov} output:

  \begin{tabular}{ll}
    \T{l-}  & '\T{-}' synthesis sign \\
    \T{l+}  & '\T{+}'  \\
    \T{l.}  & '\T{.}'  \\
    \T{l\#} & '\T{\#}' \\
    \T{l*}  & '\T{*}'  \\
  \end{tabular}

\item \T{<expected report notes>} is a comma separated sequence of coverage
  violation diagnostics expected for the selected lines in the
  \T{--annotate=report} output:
  
  \begin{tabular}{ll}
    \T{0}   & no diagnostic \\
    \T{s-}  & "statement not covered" \\
    \T{dT-} & "decision outcome True not covered" \\
    \T{dF-} & "decision outcome False not covered" \\
    \T{d!}  & "one decision outcome not covered" \\
    \T{c!}  & "independent influence of condition not demonstrated"  \\
    \T{x0}  & "exempted region, 0 exemptions" \\
    \T{x+}  & "exempted region, > 0 exemptions" \\
  \end{tabular}

  Some of these notes require more precise source location designations
  (e.g. a line segment to denote a specific condition). This can be achieved
  with a :"line segment text" extension to the note.
\end{Itemize}

The rationale for introducing such a circuitry, compared for example to
simple comparison with pre-filed expected reports, is twofold:

\begin{Itemize}
\item%
  At the cost of a bit of development in the testsuite engine, it brings
  a lot of flexibility to accomodate minor changes in output formatting
  or line numbers in test cases;
%
\item%
  It involves developers very actively in the expectations specification
  process.  

  This point is seconded by having the expected .xcov notes specified
  eventhough the \T{--annotate=xcov} output is not qualified per se: that
  introduces a specification variant.
  %
  This actually also reinforces the confidence we have in the tool qualified
  output, as checks over it are redunded with expectedly equivalent checks on
  an alternate format.
\end{Itemize}
 
Below is a simple example, with a functional \T{in\_range.adb} source
with \T{-{}- \#} markers first:

\begin{verbatim}
      function In_Range (X , Min, Max : Integer) return Boolean is
      begin
         if X < Min then     -- # XcmpMin
            return False;    -- # XoutMin
         elsif X > Max then  -- # XcmpMax
            return False;    -- # XoutMax
         else
            return True;     -- # Xin
         end if;
      end;
\end{verbatim}

Then an associated test driver with expectations referencing the functional
lines with markers:

\begin{verbatim}
      --  Exercize X > max only. Verify that the < min exit and the
      --  in-range case are reported uncovered.

      procedure Test_In_Range_GTmax is
      begin
         Assert (not In_Range (4, 2, 3));
      end;

      --# in_range.adb
      --  /XcmpMin/  l+ 0
      --  /XoutMin/  l- s-
      --  /XcmpMax/  l+ 0
      --  /XoutMax/  l+ 0
      --  /Xin/      l- s-
\end{verbatim}

The \T{<expected .xcov note>} column (2nd) for \T{in\_range.adb} states
that we expect a \T{-{}-annotate=xcov} output with:

\begin{verbatim}
      expected notes here
        v
      1 .: function In_Range (X , Min, Max : Integer) return Boolean is
      2 .: begin
      3 +:    if X < Min then     -- # XcmpMin
      4 -:       return False;    -- # XoutMin
      5 +:    elsif X > Max then  -- # XcmpMax
      6 +:       return False;    -- # XoutMax
      7 .:    else
      8 -:       return True;     -- # Xin
      9 .:    end if;
     10 .: end;
\end{verbatim}

The \T{<expected report notes>} column (3rd) indicates we're expecting
"statement not covered" diagnostics for lines 4 and 8 out of \T{xcov
  --annotate=report}.

\subsection {More on expectations semantics}

The essential purpose of the qualification process is to make sure that
improperly covered items are reported as such.
%
On this ground, the testsuite enforces stricter checks for '!' and '-' items
than for '+':

\begin{Itemize}
\item%
  For '-' or '!' items, there must be an exact match between the stated
  expectations and results reported by xcov (in both the .xcov and report
  outputs).
  %
  In other words, every expectation must be found in the tool outputs, and
  every occurrence in the tool output must have a corresponding expectation.

  This makes sure that expectations are specified carefully and that the
  tool reports exactly what we expect.
%
\item%
  For 'l+' items (.xcov outputs only), only the first of the previously
  described checks applies: absence of an expectation statement for
  '+' on a line doesn't cause a test failure.
\end{Itemize}

\T{/regexp/} filters that select no lines are allowed and act as a
no-ops. This is useful in situations where a single driver is shared across
different tests.
%
Non-empty intersections between different filters are "allowed" as well but
eventhough sometimes convenient, they most often correspond to mistakes.
%
The sets of expected indications just accumulate.

% **************************************************************************
\chapter{Qualification Data}

\section{Overview}
\label{sec:qd-overview}

\xcov{}'s qualification data materializes as a set of files and documents that
come with every delivery, summarized in the table below.

The Tool Qualification, Software Configuration Management and Software Quality
Assurance Plans are generic qualification data, \emph{not} specific to a given
release of \xcov{}.
%
All the other files and documents are specific to a given \xcov{}
release.

\begin{tabular}{|c|c|c|}
\hline
\textbf{Data} & \textbf{Provided by} & \textbf{File} \\ \hline\hline
Tool Qualification Plan & Qualification Team & tqp.pdf \\ \hline
Software Configuration Management Plan & Qualification Team & scmp.pdf \\ \hline
Software Quality Assurance Plan & Quality Assurance Team & sqap.pdf \\ \hline
Tool Operational Requirements & Development Team & tor\_tc\_ts.pdf \\ \hline
Test Cases \& Tests & " & " \\ \hline
Tests Results & Development Team & ts\_results.pdf \\ \hline
Delivery file & Qualification Team & section \ref{sec:delivery} \\ \hline
Verification results analysis & User & user-specific \\ \hline
\xcov{} Fundamentals \& User's Guide & Development Team & xcov\_ug.pdf \\ \hline
\end{tabular}

\section{Tool Qualification Plan}

This document.

\section{Software Configuration Management Plan}

TBC

\section{Software Quality Assurance Plan}

TBC

\section{Operational Requirements, Test Cases and Tests}

TBC

\section{Test Results}

TBC

\section{Delivery file}
\label{sec:delivery}

Each delivery qualified for use to produce certification evidence includes a
\T{delivery.txt} synthesis file which contains:

\begin{Itemize} 
\item%
  The description of the qualification environment, as defined by the section
  \ref{sec:equivalence} to allow equivalence checks by users;
  %
\item%
  The version of all the files listed in section \ref{sec:qd-overview}, as
  part of the qualification data;
  %
\item%
  A list of the known problems in this particular delivery;
  %
\item%
  A description of the GNAT Pro compilation options and coding standard that
  need to be obeyed, delivery specific part of the tool qualified interface.
\end{Itemize}

% **************************************************************************
\chapter{Preparing for Qualified Use}

The user needs to perform a set of activities, either in the scope of \xcov{}
qualification or \xcov{} usage.

\section{\xcov{} qualification}

To finalise the qualification of \xcov{}, the user needs to:

\begin{enumerate}
%
\item \textbf{Reference \xcov{} in the PSAC}, identifying \xcov{} as a
  verification tool requiring qualification because it contributes to take
  certification credit by automating some (otherwise manual) activities.
%
\item \textbf{Assess the equivalence between the qualification environment and
  the operational environment} (see section \ref{sec:equivalence}).
%
\item \textbf{Put qualification data under configuration management}.
\end{enumerate}

\section{\xcov{} usage}

Users needs to perform the following activities to assure \xcov{} exhibits
a qualified behaviour:

\begin{enumerate}
\item \textbf{Install \xcov{} in the Operational Environment};
%
\item \textbf{Enforce use of \xcov{}'s qualified interface}, described
  in section \ref{sec:qual-interface} of this document;
%
\item \textbf{Acknowledge and account for the known problems} documented in
  the delivery file.
%
\item \textbf{Update the Environment Configuration Index} by including the
  delivery file and test results (see section \ref{sec:qd-overview} for the
  corresponding files).
%
\item \textbf{Update the Software Accomplishment Summary} for:
\begin{itemize}
\item Objectives 5, 6 and 7 of table A7 for level A software.
\item Objectives 6 and 7 of table A7 for level B software.
\item Objective 7 of table A7 for level C software.
\end{itemize}
%
\item \textbf{Update the Software Accomplishment Summary} for the
  qualification status of \xcov{}.
\end{enumerate}

\section{Environment equivalence}
\label{sec:equivalence}

For the whole set of qualification material to be consistent, the
qualification and user environments must be equivalent.
%
Given our internal knowledge of \xcov{}, we deem the following data sufficient
to establish equivalence of environments:

\begin{enumerate}
\item the name of the GNAT Pro executable;
\item the GNAT Pro version number;
\item the \xcov{} version number;
\item the host operating system and its version;
\end{enumerate}

The value for all the items above for the qualification environment are
contained in the delivery file (section \ref{sec:delivery}). Test execution
results are produced in an environment which is equivalent to the one
described in the delivery file.

\end{document}
