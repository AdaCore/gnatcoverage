============================================
== Qualification Testsuite for Couverture ==
============================================

The Couverture qualification suite is a set of tests designed to assess
whether the tools are trustworthy enough for use in a DO178B certification
context.

The assessment is performed by examining a synthetic report produced by an
automated execution of all the tests, customized for a target certification
level (A, B or C).

Our testsuite automated execution is python driven. Each test is materalized
by a subdirectory where a "test.py" script resides, and the general suite
execution scheme is like:

 suite-toplevel-dir $ ./testsuite.py [options]
 ... scan subdirs with local test.py, run them, produce report.

For a list a infrastructure preriquisites to allow the tests to run, see the
README file in the suite toplevel dir.

============================
== Operational statements ==
============================

We qualify a precise use of a well identified xcov/qemu bundle to perform
structural coverage assessments in various areas:

 - instruction or branch coverage of machine object code,
 - C or Ada source coverage, according to the criteria required by the
   target qualification level.

We focus on the "report" text output of xcov, which exposes a list of coverage
deficiencies with respect to a given criterion, diagnostics such as "statement
not covered at <file>:<line>:<col>" or alike.

The suite driver uses the PATH environment variable to select the tools and
reports the corresponding versions. This is always relevant for xcov and qemu.

The compiler version and command line options are reported as well, which is
relevant if the binary form of the user applicative code used for coverage
assessement is to be embedded "as-is".

Proper assessment of the DC and MCDC criteria rely on the following
assumptions:

 - No exception propagation,
 - No threads,
 - Binary boolean operators all operate with short-circuit semantics.

The current set of tests is tailored accordingly.

==============================
== Source Coverage criteria ==
==============================

General organization
--------------------

The tests are hosted in the Qualif/<lang> directory subtree, with three
first level subdirectories:

 * 'stmt' for tests aimed at verifying statement coverage capabilities,
 * 'decision' for tests oriented towards decision coverage capabilities,
 * 'mcdc' for tests validating mcdc coverage assessments.

Qualification for one Do178 level requires execution of all the relevant
categories of tests: 

 * stmt for level C;
 * stmt & decision for level B;
 * stmt, decision & mcdc for level A.

Each test has a dedicated subdirectory with

  * A req.txt file, textual description of the requirement to
    be verified by the test,
  * A test.py script to drive the test execution,
  * A src subdirectory with the sources to be used for the test.

We distinguish two categories of sources:

  * functional sources, code for which some coverage properties are
    to be assessed,

  * driver sources, which invoke the functional code in various ways
    and embed a description of the expected coverage outcome.

File names starting with "test_" identify driver sources. Multiple drivers may
be used to exercise a single functional source. There may be several sets of
(functional code / drivers) associations in a single "src" directory.

Thanks to the simple source naming conventions and the in-source embedded
expectations, the test.py contents is entirely generic and can simply be
copied from one test to the other.  The presence of a test.py is still useful
to help the toplevel driver locate test directories.

Besides, the source of each driver features comments before the main
subprogram body, describing the test intent and strategy to verify [part of]
the requirement. This way, every drivers consitutes a 'testcase' for its
requirement in DO178 parlance.


Operational mode (aka 'test procedure')
---------------------------------------


For every test_<x> driver, the suite
  
  * builds the executable program (driver + functional code to exercize),

  * executes the program in qemu, producing an execution trace,

  * invokes xcov to analyze the trace and output source coverage reports,
    once with --annotate=xcov and once with --annotate=report,

  * compare the outputs with the expectations stated in the driver sources,

  * decide whether they match (test passes) or not (test fails) and report.

All of this is performed in a separate subdirectory called tmp_<x>.

Expectations micro-language
---------------------------

The description of expectations on coverage results is achieved with two
devices:

1) In functional sources:

  Comments starting with "-- #" on lines for which coverage
  expectations need to be specified. These provide ways to refer
  to functional lines from ...

2) In test sources, at the end, sequence of:

  --# <functional_file_name>      followed by optional sequence of:

  --  /regexp/ <expected .xcov note> <expected report notes>

  where

  * /regexp/ selects all the source lines matching "-- # " & regexp

  * <expected .xcov note> denotes the synthesis sign we expect on the
    selected lines in the --annotate=xcov output:

    l-  : '-' synthesis sign
    l+  : '+'     "       " 
    l.  : '.'     "       " 

  * <expected report notes> is a comma separated sequence of coverage
    deficiency diagnostics expected for the selected lines in the
    --annotate=report output:

    0   : no diagnostic
    s-  : "statement not covered" diagnostic
    s!  : "statement partially covered" diagnostic
    dT- : "decision outcome True not covered"
    dF- : "decision outcome False not covered"
    d!  : "one decision outcome not covered"
    c!  : "independent influence of condition not demonstrated"

  Some of these notes require more precise source location designations
  (e.g. a line segment to denote a specific condition). This can be achieved
  with a :"linetext" extension to the note.

Basic Example
-------------

Ada/stmt/IsolatedContructs/CompoundS/IfStatements

  * req.txt:
  "if" statements are correctly handled for statement coverage purposes.

  * test.py:
  from test_utils import *
  ExerciseAll()
  thistest.result()

  * src/
    - in_range.ad[bs]            functional code, with -- # line markers:

      function In_Range (X , Min, Max : Integer) return Boolean is
      begin
	 if X < Min then     -- # XcmpMin
	    return False;    -- # XoutMin
	 elsif X > Max then  -- # XcmpMax
	    return False;    -- # XoutMax
	 else
	    return True;     -- # Xin
	 end if;
      end;

    - test_in_range_gtmax.adb   test driver, with procedural comments
                                and expectations referencing the lines in
                                "in_range.adb" via their markers:

      with In_Range, Support; use Support;

      --  Exercize X > max only. Verify that the < min exit and the
      --  in-range case are reported uncovered.

      procedure Test_In_Range_GTmax is
      begin
	 Assert (not In_Range (4, 2, 3));
      end;

      --# in_range.adb
      --  /XcmpMin/  l+ 0
      --  /XoutMin/  l- s-
      --  /XcmpMax/  l+ 0
      --  /XoutMax/  l+ 0
      --  /Xin/      l- s-

The <expected .xcov note> column (2nd) for in_range.adb indicates that we're
expecting an annotated .adb.xcov output (out of xcov --annotate=xcov) like:

  expected notes here
        v
      1 .: function In_Range (X , Min, Max : Integer) return Boolean is
      2 .: begin
      3 +:    if X < Min then     -- # XcmpMin
      4 -:       return False;    -- # XoutMin
      5 +:    elsif X > Max then  -- # XcmpMax
      6 +:       return False;    -- # XoutMax
      7 .:    else
      8 -:       return True;     -- # Xin
      9 .:    end if;
     10 .: end;

The <expected report notes> column (3rd) indicates we're expecting "statement
not covered" diagnostics for lines 4 and 8 out of xcov --annotate=report.

Note that the "report" output only reflects '-' or '!' kinds of items.

Writing a simple test
---------------------

Create a directory at the proper place and populate it with
  
   - req.txt,
   - test.py,
   - "src" subdir with one or several sets of
       [functional unit x (.ads,.adb)
        >= 1 test_x_bla drivers with embedded expectations and comments
             on what the test does with respect to the requirement]

Then do test, first with a selective run of your new test only
(filter argument to the toplevel testsuite.py).

More on expectations semantics
------------------------------

The essential purpose of the qualification process is to make sure that
improperly covered items are reported as such. On this ground, the testsuite
enforces stricter checks for '!' and '-' items than for '+':

  * For '-' or '!' items, there must be an exact match between the stated
    expectations and results reported by xcov (in both the .xcov and report
    outputs):

    - Every expectation must be found in the coverage analysis output, and
    - Every occurrence in the output must have a corresponding expectation

    This makes sure that expectations are specified carefully and that the
    tool reports exactly what we expect.

  * For 'l+' items (.xcov outputs only), only the first check above applies.
    Absence of an expectation statement for '+' on a line doesn't cause a test
    failure.

/regexp/ filters that select no lines are allowed and act as a no-ops. This is
useful in situations where a single driver is shared across different tests.

Non-empty intersections between different filters are "allowed" as well but
most often correspond to mistakes. The sets of expected indications just
accumulate.

Guidelines
----------

Prefer simple functional sources and drivers, instead of big bundles in
sources. This facilitates maintenance (e.g. analyzing errors) and simplifies
expectation statements.

=====================
== Troubleshooting ==
=====================

If anything goes unexpected, examine test.py.err, test.py.out, test.pt.log in
test directories.
